{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  # Text classification for reuters21578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is description of the Reuter topic classification using CNN networks. \n",
    "We briefly talk about the network and implementation. Here is general graph of the network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./graph.png\" height=\"500\" width=\"500\" alt=\"Alt text that describes the graphic\" title=\"Model grapth\" />\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input layer \n",
    "\n",
    "The input layer are word embeddings. We tried embedded training along with initializing of the embedding with Glove pre trained embeddings. We do not retrain embeddings if initialized with glove. This is du to limiting number of the system parameter and study of classification as a result of last layer. As we expected, the mail result of initializing embeddings with Glove, is speed of convergence for topics with enough training data and marginal improvement of precision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolution and max-pooling\n",
    "\n",
    "Different window sizes that are used in 1-D convolution and max-pooling after that are defined here. Number and size of the filters are basically memory of the network and should be defined according to complexity of the pattern to be detected by the network. \n",
    "We used 32 filters of length 1,2 and 3 for topic with big number of news (e.g. acq) and 4 filters of the same length for topic with less news (e.g. rice). One can easily observe that network gets overtrained if we use more filters even before finishing the first epoch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## soft-max output layer\n",
    "\n",
    "This layer does the classification based on the output of the filters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we mentioned earlier. The network parameters should be adjusted for each topic to avoid overtraining. Here we show how one can run training and test for topics with good amount of news.\n",
    "To run the code, one would need to download Glove embeddings from <a href=\"http://nlp.stanford.edu/data/glove.42B.300d.zip\"> here</a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reuters21578 import Reuters\n",
    "from train import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To save time, we have dumped the dataset into a csv file. removing argument of `load_data` results in loading of data from the sgm files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_set = Reuters()\n",
    "data_set.load_data('offline')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = 'acq'\n",
    "train_positives, train_negatives = data_set.get_text(topic, 'TRAIN')\n",
    "test_positives, test_negatives = data_set.get_text(topic, 'TEST')\n",
    "main(train_positives, train_negatives, test_positives, test_negatives, topic)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance \n",
    "NN are considered state-of-the-art technology for many of NLP tasks including text classification. As we showed in <a href=\"http://localhost:8889/notebooks/reuters_text_classification.ipynb\"> TFIDF-SVM classification</a>, appart from the top 2 topics with most news, classification for other topics are not performing well. E.g. `corn` 50% precision with only 3% recall. \n",
    "Using CNN, we can reach higher than 95% precision with recall close to 100%. For topics with small test set (e.g. 25 test sentence for `rice`) these number are not statistically significant though.\n",
    "Below is training curve for `acq` taken from tensorboard\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./training.png\" height=\"500\" width=\"800\" alt=\"Alt text that describes the graphic\" title=\"Training trend\" />\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
