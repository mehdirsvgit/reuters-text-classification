{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text classification for reuters21578"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I am describing development of a naive text classification system for reuters21578.\n",
    "\n",
    "As mentioned in the task description, the task of text classification on reuters21578 requires quite a few choice while developing. Let's look at different topics and corresponding news counts which shown as USABLE i.e. TRAIN+TEST\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from reuters21578 import Reuters\n",
    "\n",
    "dataset = Reuters()\n",
    "article_stats = dataset.get_news_stats(mode='offline')\n",
    "display(article_stats[article_stats.Set =='USABLE'].sort_values(by='Count', ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "As you see there are a few topics that have more than 100 usable news articles. We use the top 10 topics as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "selected_topics_stats = article_stats[article_stats.Set =='USABLE'].sort_values(by='Count', ascending=False).head(10)\n",
    "display(selected_topics_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The next step is to define feature extraction and classification method. We have quite a few choices here! State-of-the-art text classification systems are mostly DL-based. Considering the time limit for this particulare task and limited number of news articles, I decided to use SVM classifier on top of TF-IDF features. I am using NLTK and sklearn for TF-IDF extraction and classification respectively.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "def show_result_table(scores):\n",
    "    tbl = DataFrame(columns=['Topic', 'Precision', 'Recall'])\n",
    "    for topic in scores:\n",
    "        tbl = tbl.append({'Topic': topic,'Precision': scores[topic][0],'Recall': scores[topic][1]}, ignore_index=True)\n",
    "        \n",
    "    display(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tfidf_classifier import TFIDFClassifier\n",
    "from reuters21578 import Reuters\n",
    "\n",
    "\n",
    "selected_topics = [item[0] for item in selected_topics_stats.values.tolist()]\n",
    "data_set = Reuters()\n",
    "data_set.load_data()\n",
    "tfidf_classifier = TFIDFClassifier(data_set.get_all_train())\n",
    "print(\"Calculating and adding TFIDF...\")\n",
    "data_set.add_tfidf(tfidf_classifier)\n",
    "\n",
    "scores = dict()\n",
    "for topic in selected_topics:\n",
    "    print(\"Training classifier for :  '{}'\".format(topic))\n",
    "    X_train, Y_train = data_set.get_data(topic, 'TRAIN')\n",
    "    X_test, Y_test = data_set.get_data(topic, 'TEST')\n",
    "    tfidf_classifier.train(X_train, Y_train)\n",
    "    print(\"Testing '{}'\".format(topic))\n",
    "    scores[topic]= tfidf_classifier.get_precision_recall(X_test, Y_test)\n",
    "\n",
    "show_result_table(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see result degrades for topic with lower number of news article. There are many potential ways of exploring different feature extraction and classifications as researchers are suggesting for this task. I did not have enough time to go through published works on this task.\n",
    "For such a task, I would normally go though the data to figure out distribution of words and kind of data filtering that would help better result before trying to play wih classifiers.\n",
    "\n",
    "I would like to mention following points about this implementation \n",
    "- It is not a good practice to upload data on github along with the code. However, there is an encoding problem with file reut2-017.sgm that should be fixed before being used here. Please use the Reuters data provided along with the code. \n",
    "- This implementation assumes that all the data could be held in memory which is possible for such small data set\n",
    "- dataframe operation could be much more efficient and nicer with joint. I had an error with joint that I couldn't fix fast so I sent this implementation\n",
    "\n",
    "In General, test classification tasks are very depending on the amount of data. AS result shows, recall is not consistent even for the top 10 topics. Here are some simple variations that would be nice to try\n",
    "\n",
    "- concatenating uni-gram and bi-gram TFIDF in feature vector. I expect most of the bi-gram TFIDF should be filtered out\n",
    "- It is easier to capture higher order n-gram sequences with correct length and number of filters. CNN with small filter length (e.g. [1,2]) could be useful for top 10 topics with highest number of articles\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
